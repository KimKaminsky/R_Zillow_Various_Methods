# R_Zillow_Various_Methods
This project used R to create multiple linear regression, boosted regression, ARIMA, Artificial Neural Networks, and XGBoost for the Zillow Housing competition.  Decision trees were used to impute missing values.

The object of this competition was to build a model that minimized the mean absolute error in predicting the log error of the Zestimate of the property sale price. Some basic data cleanup which included imputing missing values using decision trees and trying out a variety of models yielded a decent result, which placed 2485 in the Kaggle competition. This beat out 1400 other competitors. The model that provided the best results on the test set submitted to Kaggle was the Boosted Regression model which makes sense as this type of model seeks to minimize error through building a series of weak trees and this allows it to produce a good model without too much feature engineering. XGBoost, which often produces good results in competitions, and performed better than the Boosted Regression in the training data did not provide the best results on the test data. This may be due to overfitting. In the future, using a cross validated design could help with this along with early stopping. However, I did this competition for a time series course and hadn't learned as much about cross validation at that point in time. In the future, I will use this appraoch. Overall, all the models did a good job with an MAE ranging from about .066 to .6489. Which means there isnâ€™t a great deal of difference between the models. All the models can likely be improved either through feature engineering, better training, or parameter tuning. In order to make a definitive conclusion about which type of model performs best on this type of problem, a great deal more rigor and experimentation would be needed.
